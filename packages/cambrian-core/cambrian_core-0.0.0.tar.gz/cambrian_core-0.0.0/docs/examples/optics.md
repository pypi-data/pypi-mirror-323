# Optics

## Imaging Model 

Our model consists of a lens described by a phase mask, refractive index, and amplitude modulating (aperture) elements. We model a retina emulated by a discretized pixel at the sensor plane at a focal length distance away from the pupil.

```{figure} assets/optics/imaging.png
:align: center
:class: example-figure
:width: 75%
**Agent's imaging model.** Our simulation implements both wave and geometric optics using a OpenGL **(a)** Our scene imaging model shows how a depth dependent blur kernel is derived: light from a point source in the 3D scene propagates through free space, passing through a pupil plane which is composed of (1) an aperture with a variable aperture radius ($r$) and (2) a programmable phase mask with height map $H(x,y)$ and refractive index $n$, before forming an image on the sensor plane. (b) The approximated model uses a 2D convolution between the scene, a depth map, and a single blur kernel using the far-field approximation (i.e., the Point Spread Function) for computational efficiency.
```

All imaging systems capture the scene as an optically encoded image on to the sensor plane. These optical encodings are commonly referred to as the blur or point spread function (PSF), and are dependent on the phase and amplitude of the pupil function along with wavelength and depth of the scene point. We follow the wave propagation model described in to estimate the depth in-dependent PSF. 

Given a point light source at a distance $z$ and the pupil function $P(x,y) = A \exp(i\phi)$ the response of the agent's eye can be measure by the PSF. The PSF at the sensor plane $s$ distance away from the pupil plane is described as:

```{math}
PSF_{\lambda,_z}(x', y') = \left| \mathcal{F}^{-1} \left\{ \mathcal{F} \left\{ P(x, y) U_{in}(x, y) \right\} H_s(f_x, f_y) \right\} \right|^2,
```

where $H_s(\cdot)$ represents the field propagation transfer function for distance $s$ with $(f_x, f_y)$ as the spatial frequencies given as

```{math}
H_s(f_x, f_y) = \exp\left[ iks \sqrt{1 - (\lambda f_x)^2 - (\lambda f_y)^2} \right];
```

where \( k = $\frac{2\pi}{\lambda}$ \) is the wavenumber; $U_{in}(x,y)$ denotes the complex-valued wave field immediately before the lens which for a point light source is given as

```{math}
U_{in}(x, y) = \exp\left(ik\sqrt{x^2 + y^2 + z^2}\right);
```

$\mathcal{F} \{\cdot\}$ is the 2D Fourier transform; $(x', y')$ are the spatial coordinates on the camera plane, and $(x, y)$ are the coordinates on the lens plane. 

The phase modulation function $t_{\phi}(x, y) = e^{i \frac{2\pi}{\lambda} \phi(x, y)}$ is generated by the lens surface profile $\phi(x,y)$ which in our case is a square 2D phase-mask array of size 25 pixels that is mutated by the outer evolution loop, where $\phi(x,y) \in \{0, 0.2, 0.4, 0.6, 0.8, 1.0 \}$. These values are scaled appropriately based on the agent eye's refractive index.

Finally, our agent's image formation follows a shift-invariant convolution of the image and the depth-independent PSF to yield the final image, $I_{\ell}$, perceived by the agent. 
    
```{math}
I_{\ell} = \mathcal{S}_{\ell}(H_{\ell} \ast X_{\ell}) + N_{\ell},
```

where the sub-index $\ell$ denotes the color channel; $X_{\ell} \in \mathbb{R}^{w \times h}_{+}$ represents the underlying scene with $w \times h$ pixels; $H_{\ell}$ represents the discretized version of the PSF; $N_{\ell} \in \mathbb{R}^{w \times h}$ denotes the Gaussian noise in the sensor; $\mathcal{S}_{\ell}(\cdot): \mathbb{R}^{w \times h} \rightarrow \mathbb{R}^{w \times h}$ is the camera response function, modeled as a linear operator; and $\ast$ denotes the 2D convolution operation. 

In practice, the discretized version of the PSF is of size $(H+1, W+1)$ where $(H, W)$ is the resolution of the agent's eye $I_{\ell}$ as chosen by the evolutionary search. This is an explicit choice to make the PSF larger than the image to enable a full blur on the eye when the aperture is fully open. The scene image $X_{\ell}$ is rendered by padding $I_{\ell}$ of size $(H, W)$ to $\left( H + (2*\frac{H+1}{2}), W + (2*\frac{W+1}{2}) \right)$. This enables the corner pixels to accumulate light from areas directly due to the aperture size which is more physically-based. This also means that closing the aperture also helps with reducing the total effective field of view of the agent's eye, which is how the agent controls the blur. For a pinhole eye, the field-of-view becomes equivalent to the encoded fov in the agent's morphological gene.

## Using the Imaging Model

You can override the eye configuration to use the optics model either by setting it via the command line or directly in a yaml configuration file. We provide an example of how to create a simple custom app in {src}`tools/optics/ <tools/optics>`.

You can run this tool using the following command:

```bash
python tools/optics/optics_sweep.py
```

```{video} assets/optics/circular_aperture_sweep.mp4
:align: center
:class: example-figure
:figwidth: 75%
:loop:
:autoplay:
:muted:
:caption: In this example, we sweep various aperture radii to demonstrate the relative bluring for different aperture size values. The radius is essentially a percentage that the aperture is open; a value of 0.1 corresponds to a 10% open aperture and 1.0 corresponds to a fully open aperture.
```

The script source used to generate this video is shown below:

````{dropdown} Python Script
```{literalinclude} ../../tools/optics/optics_sweep.py
```
````

This will load the following configuration file:

````{dropdown} YAML Configuration
```{literalinclude} ../../tools/optics/configs/optics_sweep.yaml
```
````

### Adding Realism

Although the the objects and scene are still somewhat visible, in reality the scene would be much more difficult to make out. This is because as you decrease aperture size, the total light throughput would proportionally decrease. This means the overall SNR would increase. By default, the light throughput adjustment is disabled. We can enable it and add some noise to make things more realistic.

```bash
python tools/optics/optics_sweep.py \
    env.agents.agent.eyes.eye.single_eye.scale_intensity=True \
    env.agents.agent.eyes.eye.single_eye.noise_std=0.1
```

```{video} assets/optics/circular_aperture_sweep_realistic.mp4
:align: center
:class: example-figure
:figwidth: 75%
:loop:
:autoplay:
:muted:
:caption: You can see how at very small aperture values, it's almost impossible to make out the scene. Thus, there is a trade-off between light throughput and image sharpness. This is even with artificially increased intensity for sake of visualization (see the example script).
```

### Using a Random Mask

The above examples use a radially symmetric aperture (see {class}`~cambrian.eyes.optics.MjCambrianCircularApertureConfig`). However, we can directly specify a mask via the {class}`~cambrian.eyes.optics.MjCambrianMaskApertureConfig` class. This mask can be any size up to the rendered image size, and can produce some interesting looking blurring.

```bash
python tools/optics/optics_sweep.py \
    env/agents/eyes/aperture@env.agents.agent.eyes.eye.single_eye.aperture=random_mask
```

```{video} assets/optics/mask_aperture_sweep.mp4
:align: center
:class: example-figure
:figwidth: 75%
:loop:
:autoplay:
:muted:
:caption: We randomly initialize a new mask 5 times a second as an example. We use a cubic interpolation to calculate the aperture in image space.
```

## Training an Agent with Optics

We can then train an agent with optics to assess how well it performs with blurring. We can use the following command to train an agent with optics:

```bash
bash scripts/run.sh cambrian/main.py --train \
    example=detection \
    env/agents/eyes@env.agents.agent.eyes.eye.single_eye=optics \
    env.agents.agent.eyes.eye.single_eye.aperture.radius=0.75
```

```{video} assets/optics/trained_agent_r0p75.mp4
:align: center
:class: example-figure
:figwidth: 75%
:loop:
:autoplay:
:muted:
:caption: Trained with a blurred observation, the agent does learn to navigate to objects but it fails to discern the goal from the adversarial object.
```