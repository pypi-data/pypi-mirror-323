from __future__ import annotations
from typing import Any, List
from typing_extensions import override
import logging
import re

from ..vectorestore.vectorStore import VectorStore
from ..embeddings.embeddingsModel import EmbeddingsModel
from ..rag.rag import RAG
from ..llm.llm import LLM
from ..config.settings import Settings


class RAT(RAG):
    """
    Retrieval-Augmented Thinking (RAT) class that extends the Retrieval-Augmented Generation (RAG) pipeline
    to incorporate a reasoning step using a dedicated reasoning LLM.

    Attributes:
        reasoning_llm (LLM): The LLM used for reasoning and generating reflections.
    """

    def __init__(
        self,
        embedding_model: EmbeddingsModel,
        vector_store: VectorStore,
        reasoning_llm: LLM,
        llm: LLM,
    ) -> None:
        """
        Initializes the RAT pipeline with the required components.

        Args:
            embedding_model (EmbeddingsModel): The embeddings model for vectorization.
            vector_store (VectorStore): The vector store for document retrieval.
            reasoning_llm (LLM): The LLM used for reasoning.
            llm (LLM): The LLM used for generating the final answer.
        """
        super().__init__(embedding_model, vector_store, llm)
        self.reasoning_llm: LLM = reasoning_llm

    def think(self, input: str, iterations: int = 1) -> str:
        """
        Generates iterative reasoning or reflection based on the input question and retrieved documents.

        Args:
            input (str): The initial question or query provided by the user.
            iterations (int): The number of reasoning iterations to perform. Defaults to 1.

        Returns:
            str: A final reflection or reasoning generated by the reasoning LLM.
                 If no reasoning is found, returns the last valid reflection or a default message.
        """
        reflection = input
        for _ in range(iterations):
            retrieved_docs = self.retrieve({"question": reflection})
            docs_content = "\n\n".join(
                doc.page_content for doc in retrieved_docs["context"]
            )
            prompt_json = {
                "question": input,
                "context": docs_content,
                "reflection": reflection,
            }
            response = self.reasoning_llm.generate(prompt_json)
            think = re.findall(Settings.THINKING_PATTERN, response, re.DOTALL)
            if not think:
                logging.warning("No reasoning found in the LLM response.")
                return reflection or "No reasoning available."
            reflection = f"Reflexion about the problem: {think[0]}"
        return f"Reflection about the problem: {reflection}"

    @override
    def question_graph(self, question: str) -> str:
        """
        Executes the RAT pipeline for a given question by first generating a reflection and
        then invoking the state graph to generate a final answer.

        Args:
            question (str): The input question or query.

        Returns:
            str: The generated answer from the pipeline, which incorporates both
                 the initial question and the reasoning generated by the `think` method.
        """
        state = {"question": question, "reflection": self.think(question)}
        response = self.graph.invoke(state)
        return response["answer"]
