# mikegrad

This repository is an implementation of Andrej Karpathy's ["Neural Networks: Zero to Hero - Micrograd"](https://www.youtube.com/watch?v=VMj-3S1tku0) video. The project implements backpropagation from scratch.

## Features

- Autograd engine for automatic differentiation
- Backpropagation implementation
- Basic neural network components
- Sigmoid activation function
- Example training with toy regression data

## TODO

- Refactor the codebase
- Publish to PyPI
- More activation functions
- More loss functions
- Support common optimizers (SGD, Adam)
- Add more examples
