"""
Base embedding provider interface for Pepperpy.
"""
from abc import abstractmethod
from typing import Any, Dict, List, Optional, Union, Sequence, TypeVar, cast

import numpy as np
from numpy.typing import NDArray

from pepperpy.core.utils.errors import ProviderError
from pepperpy.providers.base.provider import BaseProvider

# Type aliases
Embedding = List[float]
EmbeddingBatch = List[Embedding]
FloatArray = NDArray[np.float64]


class EmbeddingProvider(BaseProvider):
    """Base class for embedding providers."""

    @abstractmethod
    async def embed_text(self, text: str) -> Embedding:
        """Generate embeddings for a single text.
        
        Args:
            text: Text to generate embeddings for.
            
        Returns:
            Vector representation of the text.
            
        Raises:
            ProviderError: If embedding generation fails.
        """
        raise NotImplementedError

    @abstractmethod
    async def embed_texts(self, texts: List[str]) -> EmbeddingBatch:
        """Generate embeddings for multiple texts.
        
        Args:
            texts: List of texts to generate embeddings for.
            
        Returns:
            List of vector representations.
            
        Raises:
            ProviderError: If embedding generation fails.
        """
        raise NotImplementedError

    @abstractmethod
    async def embed_query(self, query: str) -> Embedding:
        """Generate embeddings for a query.
        
        Some embedding models have different embeddings for queries vs documents.
        
        Args:
            query: Query text to generate embeddings for.
            
        Returns:
            Vector representation of the query.
            
        Raises:
            ProviderError: If embedding generation fails.
        """
        raise NotImplementedError

    @abstractmethod
    async def get_embedding_dimension(self) -> int:
        """Get the dimension of the embeddings generated by this provider.
        
        Returns:
            Dimension of the embedding vectors.
        """
        raise NotImplementedError

    async def validate_embeddings(self, embeddings: Union[Embedding, EmbeddingBatch]) -> None:
        """Validate embeddings format and dimensions.
        
        Args:
            embeddings: Embeddings to validate.
            
        Raises:
            ProviderError: If embeddings are invalid.
        """
        if not embeddings:
            raise ProviderError("Empty embeddings")

        dim = await self.get_embedding_dimension()

        if isinstance(embeddings[0], (int, float)):
            # Single embedding
            embedding = cast(Embedding, embeddings)
            if len(embedding) != dim:
                raise ProviderError(
                    f"Invalid embedding dimension. Expected {dim}, got {len(embedding)}"
                )
        else:
            # Multiple embeddings
            batch = cast(EmbeddingBatch, embeddings)
            for emb in batch:
                if len(emb) != dim:
                    raise ProviderError(
                        f"Invalid embedding dimension. Expected {dim}, got {len(emb)}"
                    )

    async def cosine_similarity(
        self, embedding1: Embedding, embedding2: Embedding
    ) -> float:
        """Calculate cosine similarity between two embeddings.
        
        Args:
            embedding1: First embedding vector.
            embedding2: Second embedding vector.
            
        Returns:
            Cosine similarity score.
            
        Raises:
            ProviderError: If embeddings have different dimensions.
        """
        if len(embedding1) != len(embedding2):
            raise ProviderError("Embeddings must have same dimension")

        arr1 = np.array(embedding1, dtype=np.float64)
        arr2 = np.array(embedding2, dtype=np.float64)

        norm1 = np.linalg.norm(arr1)
        norm2 = np.linalg.norm(arr2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
            
        return float(np.dot(arr1, arr2) / (norm1 * norm2))

    async def batch_cosine_similarity(
        self, query_embedding: Embedding, document_embeddings: EmbeddingBatch
    ) -> List[float]:
        """Calculate cosine similarities between a query and multiple documents.
        
        Args:
            query_embedding: Query embedding vector.
            document_embeddings: List of document embedding vectors.
            
        Returns:
            List of cosine similarity scores.
            
        Raises:
            ProviderError: If embeddings have different dimensions.
        """
        scores = []
        for doc_emb in document_embeddings:
            score = await self.cosine_similarity(query_embedding, doc_emb)
            scores.append(score)
        return scores

    @abstractmethod
    async def get_metadata(self) -> Dict[str, Any]:
        """Get provider metadata.
        
        Returns:
            Dictionary containing:
                - model_name: Name of the embedding model
                - dimension: Embedding dimension
                - max_length: Maximum input length (if applicable)
                - supports_batching: Whether batched embedding is supported
                - provider_name: Name of the provider service
        """
        raise NotImplementedError

    async def normalize_embedding(self, embedding: Embedding) -> Embedding:
        """Normalize an embedding vector to unit length.
        
        Args:
            embedding: Vector to normalize.
            
        Returns:
            Normalized vector.
        """
        arr = np.array(embedding, dtype=np.float64)
        norm = np.linalg.norm(arr)
        if norm == 0:
            return embedding
        normalized = (arr / norm).tolist()
        return cast(Embedding, normalized) 