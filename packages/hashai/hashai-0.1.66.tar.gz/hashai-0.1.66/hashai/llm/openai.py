from typing import List, Dict, Optional
from .base_llm import BaseLLM
from openai import OpenAI
import os

class OpenAILlm(BaseLLM):
    def __init__(self, model: str = "gpt-4", api_key: Optional[str] = None):
        """
        Initialize the OpenAI LLM class.

        Args:
            model (str): The name of the model (e.g., gpt-4, gpt-4-vision).
            api_key (Optional[str]): The OpenAI API key. If not provided, it fetches from the environment.
        """
        self.model = model
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OpenAI API key is required. Set OPENAI_API_KEY environment variable or pass it explicitly.")
        self.client = OpenAI(api_key=self.api_key)

    def generate(self, prompt: str, context: Optional[List[Dict]] = None, memory: Optional[List[Dict]] = None) -> str:
        """
        Generate text using OpenAI's ChatCompletion API.

        Args:
            prompt (str): The user prompt.
            context (Optional[List[Dict]]): Context to include in the conversation.
            memory (Optional[List[Dict]]): Memory from previous interactions.

        Returns:
            str: The generated response from the model.
        """
        try:
            # Prepare messages for the OpenAI API
            messages = []
            if memory:
                messages.extend(memory)
            if context:
                messages.append({"role": "system", "content": "Context: " + str(context)})
            messages.append({"role": "user", "content": prompt})

            # Call the ChatCompletion endpoint
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
            )

            # Correct way to access the response:
            return response['choices'][0]['message']['content']
        except Exception as e:
            raise ValueError(f"Error while generating response with OpenAI: {e}")

    @property
    def supports_vision(self) -> bool:
        """
        Check if the model supports vision tasks.

        Returns:
            bool: True if the model supports vision, False otherwise.
        """
        return self.model.startswith("gpt-4")

    def generate_from_image_url(self, image_url: str, instructions: str = "", **kwargs) -> str:
        """
        Process an image URL with OpenAI's vision-capable models, using instructions as the prompt.

        Args:
            image_url (str): The URL of the image.
            instructions (str): Instructions provided as the prompt for image analysis.
            kwargs: Additional parameters for the OpenAI API.

        Returns:
            str: The response generated by the vision-capable model.
        """
        if not self.supports_vision:
            raise ValueError(f"Model '{self.model}' does not support vision tasks.")

        try:
            # Use instructions as the prompt in the API call
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": instructions},  # Using instructions as the prompt
                            {"type": "image_url", "image_url": {"url": image_url}},
                        ],
                    }
                ],
                **kwargs,
            )
            return response['choices'][0]['message']['content']
        except Exception as e:
            raise ValueError(f"Error while processing image URL with OpenAI Vision model: {e}")

    def generate_from_image(self, image_bytes: bytes, instructions: str = "", **kwargs) -> str:
        """
        Process a locally stored image with OpenAI's vision-capable models.

        Args:
            image_bytes (bytes): The image data in bytes.
            instructions (str): Instructions provided as the prompt for image analysis.
            kwargs: Additional parameters for the OpenAI API.

        Returns:
            str: The response generated by the vision-capable model.
        """
        raise NotImplementedError("Local image support is not implemented yet.")
