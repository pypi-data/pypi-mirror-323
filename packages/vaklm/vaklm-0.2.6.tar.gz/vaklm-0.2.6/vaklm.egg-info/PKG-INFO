Metadata-Version: 2.1
Name: vaklm
Version: 0.2.6
Home-page: https://github.com/hemamth/vaklm
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
Description-Content-Type: text/markdown

# vaklm

Easy interaction with OpenAI-compatible LLM endpoints.

No more `client.chat.completions.create` verbosity!

## Installation

```bash
pip install vaklm
```

## Usage

```python
from vaklm import vaklm, VAKLMException

print("Non-streaming example:")
try:
    response = vaklm(
        endpoint="http://localhost:11434/v1/chat/completions",
        model_name="llama3.2:latest",
        user_prompt="Write a short story about a cat.",
        system_prompt="You are a creative writer.",
        api_key="YOUR_API_KEY",
        temperature=0.7
    )
    print(response)
except VAKLMException as e:
    print(f"Error: {str(e)}")
```

```python
from vaklm import vaklm

print("\nStreaming example:")
try:
    for chunk in vaklm(
        endpoint="http://localhost:11434/v1/chat/completions",
        model_name="llama3.2:latest",
        user_prompt="Write a short story about a cat.",
        system_prompt="You are a creative writer.",
        api_key="YOUR_API_KEY",
        stream=True,
        temperature=0.7
    ):
        print(chunk, end='', flush=True)
except VAKLMException as e:
    print(f"Error: {str(e)}")

# Note: When streaming, the function also accumulates any reasoning_content
# from the API response which can be accessed or processed as needed.
```
